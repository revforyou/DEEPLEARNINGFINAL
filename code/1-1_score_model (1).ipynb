{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Job Matching using Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "To match CVs to jobs using Doc2Vec, we first import needed libraries and load job data from a CSV file. We clean up the data, keep only the important columns, and combine them into a new column called 'data.' Then, we break down the words in the 'data' column and tag them with unique IDs using the TaggedDocument class.\n",
    "\n",
    "Next, we set up the Doc2Vec model with certain settings, like the size of the vector, minimum count, and number of times to run. We create the vocabulary by giving the tagged data to the model, and then train the model on this data.\n",
    "\n",
    "After training, we save the model for later use. We create perfect resumes from each job description to test our model. To match a resume with a job description, we load the saved model and clean up the resume and job description text. We clean the data, making them lowercase, and removing punctuation, and etc.\n",
    "\n",
    "Using the trained model, we get the document vectors for the resume and job description. Then, we find the cosine similarity between the two vectors to see how well the resume and the job description match. We do this for both a random resume and the perfect resume. The cosine similarity score is between -1 and 1, with 1 being a perfect match and -1 being no match at all.\n",
    "\n",
    "By using Doc2Vec and cosine similarity, we can match job descriptions and resumes quickly and effectively. This makes the job application process easier and increases the chances of finding the right person for the job.\n",
    "\n",
    "Lastly, we use a Gauge chart to show the matching percentage for both scores. This gives users a threshold that they could consider when changing their CV to pass the Application Tracking System (ATS) used by most employers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding\n",
    "#### 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YDFUjQi3S171"
   },
   "outputs": [],
   "source": [
    "## Install all dependencies\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install requests\n",
    "# !pip install PyPDF2\n",
    "# !pip install termcolor\n",
    "# !pip install python-docx\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kpZnVCxZSQ8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hh1980/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.linalg import norm\n",
    "from termcolor import colored\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare data\n",
    "Prepare data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "5CoVK6-iSWsU",
    "outputId": "20a60cf2-8224-47a2-d43c-b20d351db269"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "      <th>Easy Apply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hopper\\n3.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Travel Agencies</td>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>At Noom, we use scientifically proven methods ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Noom US\\n4.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>2008</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Health, Beauty, &amp; Fitness</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Decode_M\\n\\nhttps://www.decode-m.com/\\n\\nData ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Decode_M</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1 to 50 employees</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Sapphire Digital seeks a dynamic and driven mi...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Sapphire Digital\\n3.4</td>\n",
       "      <td>Lyndhurst, NJ</td>\n",
       "      <td>Lyndhurst, NJ</td>\n",
       "      <td>201 to 500 employees</td>\n",
       "      <td>2019</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>Zocdoc, Healthgrades</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Director, Data Science</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Director, Data Science - (200537)\\nDescription...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>United Entertainment Group\\n3.4</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>BBDO, Grey Group, Droga5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                          Job Title  \\\n",
       "0           0      0              Senior Data Scientist   \n",
       "1           1      1  Data Scientist, Product Analytics   \n",
       "2           2      2               Data Science Manager   \n",
       "3           3      3                       Data Analyst   \n",
       "4           4      4             Director, Data Science   \n",
       "\n",
       "                Salary Estimate  \\\n",
       "0  $111K-$181K (Glassdoor est.)   \n",
       "1  $111K-$181K (Glassdoor est.)   \n",
       "2  $111K-$181K (Glassdoor est.)   \n",
       "3  $111K-$181K (Glassdoor est.)   \n",
       "4  $111K-$181K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  \\\n",
       "0  ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...     3.5   \n",
       "1  At Noom, we use scientifically proven methods ...     4.5   \n",
       "2  Decode_M\\n\\nhttps://www.decode-m.com/\\n\\nData ...    -1.0   \n",
       "3  Sapphire Digital seeks a dynamic and driven mi...     3.4   \n",
       "4  Director, Data Science - (200537)\\nDescription...     3.4   \n",
       "\n",
       "                      Company Name       Location      Headquarters  \\\n",
       "0                      Hopper\\n3.5   New York, NY  Montreal, Canada   \n",
       "1                     Noom US\\n4.5   New York, NY      New York, NY   \n",
       "2                         Decode_M   New York, NY      New York, NY   \n",
       "3            Sapphire Digital\\n3.4  Lyndhurst, NJ     Lyndhurst, NJ   \n",
       "4  United Entertainment Group\\n3.4   New York, NY      New York, NY   \n",
       "\n",
       "                     Size  Founded  Type of ownership  \\\n",
       "0   501 to 1000 employees     2007  Company - Private   \n",
       "1  1001 to 5000 employees     2008  Company - Private   \n",
       "2       1 to 50 employees       -1            Unknown   \n",
       "3    201 to 500 employees     2019  Company - Private   \n",
       "4     51 to 200 employees     2007  Company - Private   \n",
       "\n",
       "                    Industry                  Sector  \\\n",
       "0            Travel Agencies        Travel & Tourism   \n",
       "1  Health, Beauty, & Fitness       Consumer Services   \n",
       "2                         -1                      -1   \n",
       "3                   Internet  Information Technology   \n",
       "4    Advertising & Marketing       Business Services   \n",
       "\n",
       "                    Revenue               Competitors Easy Apply  \n",
       "0  Unknown / Non-Applicable                        -1         -1  \n",
       "1  Unknown / Non-Applicable                        -1         -1  \n",
       "2  Unknown / Non-Applicable                        -1       True  \n",
       "3  Unknown / Non-Applicable      Zocdoc, Healthgrades         -1  \n",
       "4  Unknown / Non-Applicable  BBDO, Grey Group, Droga5         -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./dataset/DataScientist.csv')\n",
    "# Check data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since head() fuction does not show all data, we check column names to retain only necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'index', 'Job Title', 'Salary Estimate',\n",
       "       'Job Description', 'Rating', 'Company Name', 'Location', 'Headquarters',\n",
       "       'Size', 'Founded', 'Type of ownership', 'Industry', 'Sector', 'Revenue',\n",
       "       'Competitors', 'Easy Apply'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show column name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only some columns to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...</td>\n",
       "      <td>Travel Agencies</td>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>At Noom, we use scientifically proven methods ...</td>\n",
       "      <td>Health, Beauty, &amp; Fitness</td>\n",
       "      <td>Consumer Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Decode_M\\n\\nhttps://www.decode-m.com/\\n\\nData ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Sapphire Digital seeks a dynamic and driven mi...</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Director, Data Science</td>\n",
       "      <td>Director, Data Science - (200537)\\nDescription...</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Job Title  \\\n",
       "0              Senior Data Scientist   \n",
       "1  Data Scientist, Product Analytics   \n",
       "2               Data Science Manager   \n",
       "3                       Data Analyst   \n",
       "4             Director, Data Science   \n",
       "\n",
       "                                     Job Description  \\\n",
       "0  ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...   \n",
       "1  At Noom, we use scientifically proven methods ...   \n",
       "2  Decode_M\\n\\nhttps://www.decode-m.com/\\n\\nData ...   \n",
       "3  Sapphire Digital seeks a dynamic and driven mi...   \n",
       "4  Director, Data Science - (200537)\\nDescription...   \n",
       "\n",
       "                    Industry                  Sector  \n",
       "0            Travel Agencies        Travel & Tourism  \n",
       "1  Health, Beauty, & Fitness       Consumer Services  \n",
       "2                         -1                      -1  \n",
       "3                   Internet  Information Technology  \n",
       "4    Advertising & Marketing       Business Services  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df[['Job Title', 'Job Description', 'Industry', 'Sector']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>Phoenix Data Science Tutor Jobs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>Civil Engineer/GIS Data Analyst</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>Principal Device Modeling Engineer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>SQL/SAS Data Analyst</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>Patient Safety Physician or Safety Scientist -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2079 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  index  Job Title\n",
       "0                                        Data Scientist        274\n",
       "1                                         Data Engineer        260\n",
       "2                                          Data Analyst        246\n",
       "3                                 Senior Data Scientist         91\n",
       "4                                   Senior Data Analyst         47\n",
       "...                                                 ...        ...\n",
       "2074                    Phoenix Data Science Tutor Jobs          1\n",
       "2075                    Civil Engineer/GIS Data Analyst          1\n",
       "2076                 Principal Device Modeling Engineer          1\n",
       "2077                               SQL/SAS Data Analyst          1\n",
       "2078  Patient Safety Physician or Safety Scientist -...          1\n",
       "\n",
       "[2079 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = df['Job Title'].value_counts().reset_index()\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data\n",
      "0  Senior Data Scientist ABOUT HOPPER\\n\\nAt Hoppe...\n",
      "1  Data Scientist, Product Analytics At Noom, we ...\n",
      "2  Data Science Manager Decode_M\\n\\nhttps://www.d...\n",
      "3  Data Analyst Sapphire Digital seeks a dynamic ...\n",
      "4  Director, Data Science Director, Data Science ...\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called 'data' and merge the values of the other columns into it\n",
    "df['data'] = df[['Job Title', 'Job Description', 'Industry', 'Sector']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "# Drop the individual columns if you no longer need them\n",
    "df.drop(['Job Title', 'Job Description', 'Industry', 'Sector'], axis=1, inplace=True)\n",
    "# Preview the updated dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenize data\n",
    "We tokenize the words in the 'data' column and tag them with unique identifiers using the TaggedDocument class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sCAMBUD8Sorw"
   },
   "outputs": [],
   "source": [
    "# Tag data\n",
    "data = list(df['data'])\n",
    "tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model initialization and vocabulary buiding\n",
    "Next, we initialize the Doc2Vec model with specific parameters.\n",
    "\n",
    "**Parameters** of Doc2Vec are as follows: \n",
    "\n",
    "- `vector_size`: Dimensionality of the feature vectors. Default: 100.\n",
    "- `window`: The window refers to the maximum distance between the target word and its context words within a sentence. Default: 5.\n",
    "- `min_count`: Ignores all words with a total frequency lower than this. Default: 5.\n",
    "- `epochs`: Number of iterations (epochs) over the corpus. Defaults to 5 for PV-DBOW and 10 for PV-DM.\n",
    "- `dm`: Defines the training algorithm. If `dm = 1`, the Distributed Memory (PV-DM) model is used. If `dm = 0`, the Distributed Bag of Words (PV-DBOW) model is used. Default: 1 (PV-DM).\n",
    "- `dbow_words`: If set to 1, trains word vectors (in addition to document vectors) using the PV-DBOW algorithm. Default: 0 (False).\n",
    "- `dm_mean`: If set to 1, uses the mean of the context word vectors instead of concatenation when inferring vectors in the PV-DM model. Default: 0 (False).\n",
    "- `dm_concat`: If set to 1, concatenates the document and context word vectors when inferring vectors in the PV-DM model. Default: 0 (False).\n",
    "- `dm_tag_count`: Expected number of document tags per document, when using the PV-DM algorithm. Default: 1.\n",
    "- `dbow_tag_count`: Expected number of document tags per document, when using the PV-DBOW algorithm. Default: 1.\n",
    "- `alpha`: The initial learning rate. Default: 0.025.\n",
    "- `min_alpha`: The learning rate will linearly drop to `min_alpha` as training progresses. Default: 0.0001.\n",
    "- `hs`: If set to 1, hierarchical softmax activation function will be used. Default: 0 (Negative Sampling).\n",
    "- `negative`: If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn. Default: 5.\n",
    "- `ns_exponent`: The exponent used to shape the negative sampling distribution. Default: 0.75.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pN_H6onBTamK"
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = Doc2Vec(vector_size = 50,\n",
    "min_count = 5,\n",
    "epochs = 10,\n",
    "alpha = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Sv_Mtx4OWFgF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11885\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary building\n",
    "model.build_vocab(tagged_data)\n",
    "# Get the vocabulary keys\n",
    "keys = model.wv.key_to_index.keys()\n",
    "# Print the length of the vocabulary keys\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train and save the model\n",
    "Train the model on tagged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0jwx4eNAWYrI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/10\n",
      "Training epoch 2/10\n",
      "Training epoch 3/10\n",
      "Training epoch 4/10\n",
      "Training epoch 5/10\n",
      "Training epoch 6/10\n",
      "Training epoch 7/10\n",
      "Training epoch 8/10\n",
      "Training epoch 9/10\n",
      "Training epoch 10/10\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(model.epochs):\n",
    "    print(f\"Training epoch {epoch+1}/{model.epochs}\")\n",
    "    model.train(tagged_data, \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=model.epochs)\n",
    "\n",
    "model.save('cv_job_maching.model')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Generate perfect resumes for job description for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extract Job Requirements\n",
    "def extract_requirements(job_description):\n",
    "    # Use spaCy to extract skills, experience, and education\n",
    "    doc = nlp(job_description)\n",
    "    skills = extract_skills(doc)\n",
    "    experience = extract_experience(doc)\n",
    "    education = extract_education(doc)\n",
    "\n",
    "    return {\n",
    "        'skills': skills,\n",
    "        'experience': experience,\n",
    "        'education': education\n",
    "    }\n",
    "\n",
    "def extract_skills(doc):\n",
    "    # Define patterns to match skills\n",
    "    skill_patterns = [\n",
    "        [{'POS': 'NOUN'}, {'LOWER': 'skills'}],\n",
    "        [{'POS': 'NOUN'}, {'LOWER': 'experience'}, {'LOWER': 'in'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'CCONJ'}, {'POS': 'NOUN'}],  # e.g., \"Python and SQL\"\n",
    "        [{'POS': 'NOUN'}, {'POS': 'PART'}, {'POS': 'NOUN'}],   # e.g., \"Machine Learning\"\n",
    "        [{'POS': 'NOUN'}, {'POS': 'NOUN'}]                     # e.g., \"Data Analysis\"\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"SKILLS\", skill_patterns)\n",
    "\n",
    "    skills = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills.append(span.text)\n",
    "\n",
    "    # Remove duplicates and join skills with commas\n",
    "    skills = list(set(skills))\n",
    "    return ', '.join(skills)\n",
    "\n",
    "def extract_experience(doc):\n",
    "    # Define patterns to match experience requirements\n",
    "    exp_patterns = [\n",
    "        [{'LOWER': 'experience'}, {'POS': 'NUM'}, {'LOWER': 'years'}],\n",
    "        [{'LOWER': 'minimum'}, {'POS': 'NUM'}, {'LOWER': 'years'}, {'LOWER': 'experience'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"EXPERIENCE\", exp_patterns)\n",
    "\n",
    "    experience = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        experience.append(span.text)\n",
    "\n",
    "    return experience\n",
    "\n",
    "def extract_education(doc):\n",
    "    # Define patterns to match education requirements\n",
    "    edu_patterns = [\n",
    "        [{'LOWER': 'degree'}, {'POS': 'NOUN'}],\n",
    "        [{'LOWER': 'bachelor'}, {'POS': 'NOUN'}],\n",
    "        [{'LOWER': 'master'}, {'POS': 'NOUN'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"EDUCATION\", edu_patterns)\n",
    "\n",
    "    education = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        education.append(span.text)\n",
    "\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the Resume Template\n",
    "def fill_resume_template(job_description, template):\n",
    "    # Extract requirements from job description\n",
    "    requirements = extract_requirements(job_description)\n",
    "\n",
    "    # Fill the template with the requirements\n",
    "    resume = template.copy()\n",
    "    resume['Skills'] = ', '.join(requirements['skills'])\n",
    "    resume['Work Experience'] = [create_work_experience(req, requirements['skills']) for req in requirements['experience']]\n",
    "    resume['Education'] = create_education(requirements['education'][0]) if requirements['education'] else None\n",
    "\n",
    "    # Add relevant sections for data science roles\n",
    "    resume['Programming Languages'] = ', '.join(get_programming_languages(requirements['skills']))\n",
    "    resume['Data Science Skills'] = ', '.join(get_data_science_skills(requirements['skills']))\n",
    "    resume['Tools & Technologies'] = ', '.join(get_tools_and_technologies(requirements['skills']))\n",
    "\n",
    "    # Differentiate resumes based on specific requirements\n",
    "    resume['Domain Expertise'] = ', '.join(get_domain_expertise(job_description))\n",
    "    resume['Specialized Skills'] = ', '.join(get_specialized_skills(job_description))\n",
    "\n",
    "    return resume\n",
    "\n",
    "def create_work_experience(experience_requirement, skills):\n",
    "    # Parse the experience requirement\n",
    "    match = re.search(r'(\\d+)\\s*years', experience_requirement, re.I)\n",
    "    if match:\n",
    "        years = int(match.group(1))\n",
    "        position = 'Data Scientist' if 'data scientist' in experience_requirement.lower() else 'Relevant Position'\n",
    "        return {\n",
    "            'Position': position,\n",
    "            'Company': 'Relevant Company',\n",
    "            'Duration': f'{years} years',\n",
    "            'Responsibilities': ', '.join(skills)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def create_education(education_requirement):\n",
    "    # Parse the education requirement\n",
    "    degree_match = re.search(r'(bachelor|master|phd)\\s*(degree|\\'s)', education_requirement, re.I)\n",
    "    if degree_match:\n",
    "        degree_type = degree_match.group(1).capitalize()\n",
    "        return {\n",
    "            'Degree': f'{degree_type} Degree',\n",
    "            'Field': 'Data Science, Statistics, Computer Science, or related field',\n",
    "            'University': 'Relevant University'\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def get_programming_languages(skills):\n",
    "    languages = ['Python', 'R', 'SQL', 'Java', 'C++', 'C', 'Scala','SAS']\n",
    "    return [lang for lang in languages if lang.lower() in [skill.lower() for skill in skills]]\n",
    "\n",
    "def get_data_science_skills(skills):\n",
    "    data_science_skills = ['Machine Learning', 'Statistical Modeling', 'Data Analysis', 'Data Visualization',\n",
    "                           'Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning']\n",
    "    return [skill for skill in data_science_skills if any(s.lower() in skill.lower() for s in skills)]\n",
    "\n",
    "def get_tools_and_technologies(skills):\n",
    "    tools_and_technologies = ['Pandas', 'NumPy', 'Scikit-learn', 'TensorFlow', 'Spark', 'Hadoop', 'Hive', 'Impala',\n",
    "                              'MapReduce', 'Pig', 'MongoDB', 'Postgres', 'NoSQL', 'Beautifulsoup', 'Selenium', 'Scrapy',\n",
    "                              'HTML5', 'JavaScript', 'CSS', 'R Shiny', 'Tableau','SQL','R','SAS','Hive','Database']\n",
    "    return [tool for tool in tools_and_technologies if any(s.lower() in tool.lower() for s in skills)]\n",
    "\n",
    "def get_domain_expertise(job_description):\n",
    "    domains = ['Healthcare', 'Finance', 'Retail', 'Marketing', 'Telecommunications', 'Manufacturing', 'Energy',\n",
    "               'Transportation', 'Government', 'Defense', 'Security','Travel']\n",
    "    return [domain for domain in domains if domain.lower() in job_description.lower()]\n",
    "\n",
    "def get_specialized_skills(job_description):\n",
    "    specialized_skills = ['Recommender Systems', 'Natural Language Processing', 'Computer Vision', 'Time Series Analysis',\n",
    "                          'Forecasting', 'Clustering', 'Ensemble Methods', 'Bayesian Methods', 'Neural Networks',\n",
    "                          'Deep Learning', 'Reinforcement Learning', 'Collaborative Filtering', 'Web Scraping','Unix','Linux']\n",
    "    return [skill for skill in specialized_skills if any(s.lower() in job_description.lower() for s in skill.split())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Resume Template\n",
    "resume_template = {\n",
    "    'Personal Information': {\n",
    "        'Name': '',\n",
    "    },\n",
    "    'Skills': '',\n",
    "    'Work Experience': [],\n",
    "    'Education': {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Perfect Resumes\n",
    "perfect_resumes = []\n",
    "for job_description in df['data']:\n",
    "    perfect_resume = fill_resume_template(job_description, resume_template)\n",
    "    perfect_resumes.append(perfect_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "# Create a directory to save the resumes\n",
    "save_dir = 'perfect_resumes'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each perfect resume as a .docx file\n",
    "for i, resume in enumerate(perfect_resumes):\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add personal information\n",
    "    doc.add_heading(resume['Personal Information']['Name'], 0)\n",
    "    \n",
    "    # Add skills\n",
    "    doc.add_heading('Skills', level=1)\n",
    "    doc.add_paragraph(resume['Skills'])\n",
    "    \n",
    "    # Add work experience\n",
    "    doc.add_heading('Work Experience', level=1)\n",
    "    for exp in resume['Work Experience']:\n",
    "        if exp:\n",
    "            doc.add_paragraph(f\"{exp['Position']} at {exp['Company']} ({exp['Duration']})\")\n",
    "            doc.add_paragraph(exp['Responsibilities'])\n",
    "            doc.add_paragraph()\n",
    "    \n",
    "    # Add education\n",
    "    if resume['Education']:\n",
    "        doc.add_heading('Education', level=1)\n",
    "        doc.add_paragraph(f\"{resume['Education']['Degree']} in {resume['Education']['Field']}\")\n",
    "        doc.add_paragraph(f\"{resume['Education']['University']}\")\n",
    "    \n",
    "    # Add additional sections\n",
    "    if resume['Programming Languages']:\n",
    "        doc.add_heading('Programming Languages', level=1)\n",
    "        doc.add_paragraph(resume['Programming Languages'])\n",
    "    \n",
    "    if resume['Data Science Skills']:\n",
    "        doc.add_heading('Data Science Skills', level=1)\n",
    "        doc.add_paragraph(resume['Data Science Skills'])\n",
    "    \n",
    "    if resume['Tools & Technologies']:\n",
    "        doc.add_heading('Tools & Technologies', level=1)\n",
    "        doc.add_paragraph(resume['Tools & Technologies'])\n",
    "    \n",
    "    if resume['Domain Expertise']:\n",
    "        doc.add_heading('Domain Expertise', level=1)\n",
    "        doc.add_paragraph(resume['Domain Expertise'])\n",
    "    \n",
    "    if resume['Specialized Skills']:\n",
    "        doc.add_heading('Specialized Skills', level=1)\n",
    "        doc.add_paragraph(resume['Specialized Skills'])\n",
    "    \n",
    "    # Save the document\n",
    "    save_path = os.path.join(save_dir, f\"perfect_resume_{i+1}.docx\")\n",
    "    doc.save(save_path)\n",
    "    print(f\"Perfect resume {i+1} saved as {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFXCiT8GWgdP"
   },
   "source": [
    "#### 7. Inputs of CV and JD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Resume**:\n",
    "\n",
    "Get a random resume from our resume dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BK-fllx0WglI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY:17 years of IT experience currently working as Group Project Manager, Engineering/R&D with HARMAN Connected Services (a Samsung Company)  Agile/Scrum/PMP Certified PMP, Scrum and Trained Lead SaFE Agilist (4.0) with proven competence in building, training Agile Scrum teams and facilitating Scrum Masters Extensive experience in Sprint planning and estimation, tools like Poker Estimation, KANBAN Hands on experience in Change management and accommodate new business requirements by prioritizing and performing impact analysis Involved in driving the team to achieve high velocity and burn down rates  Conducted RTE activities across product lines Currently undergoing Nanodegree certification on Data Science using Python Technical Skills: Software Development/Management Domain  Expertise in business domains including Retail, Finance, Legal, ALM and B2C, catering to building grounds up applications to their maintenance. Acted as proxy product owner in the geo absence of Product Management team. Project Management/People Management Skilful in Conflict Resolution, Risk Identification and Mitigation and Performance Management & People Management Highly capable in multi-tasking and managing multiple projects for processing requirements from conceptualization to final delivery. Solid understanding of SDLC from Requirements Analysis to Post Implementation Extensive expertise in project planning, schedules, resources, cost & overall project management plan. High degree of collaborations & effective communication among stakeholders. Good knowledge handling releases and deployment via CI / CD. Conduct Scrum ceremonies; daily stand up, backlog grooming, Iteration planning, Iteration close & Retrospection. Facilitated code reviews, created best practices knowledge base, and created Lessons learnt knowledge base at the end of each sprint & project Manage project budget, time lines, quality and P&L parameters EDUCATION: Bachelor of Engineering (Electronics & Communication) 1998, .Karnataka University   India PROFESSIONAL TRAINING AND CERTIFICATIONS: Lead SAFe Agilist ~ PMP ~ Scrum professional ~Java Sun certification (2000) ~ TQMI Internal Auditor IS0 9001:2000 ~ Soft skills development PROFESSIONAL EXPERIENCE:Client: NCR Corporation, Atlanta USA                                                                        May, 2017 to till DateRole: Group Project Manager/Agile Program ManagerProject Description:  It’s a leading transaction based Hardware and software companyResponsibilities: As a Group project manager, I was responsible for delivery of the above requirements which was 25 people engagement and was spread across 4 different locations (Bangalore, OMAHA(USA), Atlanta(USA), Plano(USA)) Responsible for acting as Scrum of Scrum conducting daily meetings to review the progress made by scrum teams and helping to resolve their impediments. Involve in techno/Functional discussions and coordinate with customer to provide right solutions and clarifications; Coordinate and manage CICD deployment & releases and dependency within different teams. Estimate work efforts for project sizing, and efficient work flow with risk free technical approaches. Guided & Coached teams with Agile / Scrum Processes, managed & groomed backlog.  Administering the Jira board and presenting it to customer on alternate day basis. Managing project health through Process excellence team for providing better quality deliverables conducting code reviews, documents reviews and demos, Managing project budget, time lines, quality and P& L Parameters. Environment:  MDM, Core Java Servlets, DB Agonistic, SQL Server, DB2, HTML, JavaScript, Tomcat/Apache, WMS, .NET Framework 4.6, ASP.NET web forms. MSMQ, SQL Server 2008 R2, Pricing, Java Servlets, SQL ServerClient: CitiXsys Inc, NY                                                                         November, 2015 to May, 2017Role: Group Project Manager/Lead Agile Program ManagerProject Description: Modernizing CitiXsys core suite is highly essential due to change in technologies and use cases in retail. Modernization program objective is to have complete suite re-engineering on technology agnostic, seamless and ease of deployment, maintenance, secure and highly scalable. Responsibilities:As a Group project manager, I was responsible for delivery of the above requirements which was 55 people engagement and was spread across 4 different locations (Bangalore, Coimbatore, Russia and Delhi) Conducting meetings with customer on Technical Requirements initially, document and sign off. Responsible for acting as Scrum of Scrum conducting daily meetings to review the progress made by 4 scrum teams and helping to resolve their impediments. Involved in driving the team to achieve higher velocity & huge burn down rates in small two week Iterations. Coordinate and manage CICD deployment & releases and dependency within different teams. Estimate work efforts for project sizing, and efficient work flow with risk free technical approaches. Guided & Coached teams with Agile / Scrum Processes, managed & groomed backlog.Administrating the JIRA board and presenting it to the customer on alternate basis.Conducting monthly and quarterly business reviews with customer with agile project metrics and dashboards. Visiting customer counterparts at their location and helping team to provide effective demo and feedback and sign off. Managing project health through Process excellence team for providing better quality deliverables conducting code reviews, documents reviews and demos.Managing project budget, time lines, quality and P& L ParametersEnvironment: MVC 5.0, 4.0, ASP.NET 2.0, ASP.NET 3.5, 4.5,  .NET C#, Web Service, Sql Server 2013Client: Consensus Inc (Subsidiary of Target Corporation), San Francisco  July, 2015-November, 2015Role: Group Project Manager/Senior Delivery ManagerProject Description: The Consensus Engineering project is to maintain the existing POA application with feature enhancements. This caters to the in-house need of Target Corporation. Design and develop next generation POA and OOPIS systems. These caters to the Target sales team (in-store) and opens up a new sales channel on B2C. These applications/products will help Target to on-board customers, maintain their profiles, carrier selection, tariff plans, credit scores, insurance and other workflows of the telecom retail.Responsibilities: As a Group project manager, I was responsible for delivery of the above requirements which was 120 people engagement and was spread across 2 different locations (San Francisco & Bangalore) Responsible for acting as Scrum of Scrum conducting daily meetings to review the progress made by 8 scrum teams and helping to resolve their impediments. Involved in driving the team to achieve higher velocity & huge burn down rates in small two week Iterations. Coordinate and manage CICD deployment & releases and dependency within different teams. Estimate work efforts for project sizing, and efficient work flow with risk free technical approaches. Guided & Coached teams with Agile / Scrum Processes, managed & groomed backlog.  Administering the Jira board and presenting it to customer on alternate day basis. Managing project health through Process excellence team for providing better quality deliverables conducting code reviews, documents reviews and demosEnvironment: Development: Linux, PHP 5.x, MySQL 5.x, HTML5, CSS3, Angular JS, jQuery, AWS cloud Services Third-party Integration Libraries: Verizon, AT&T, Sprint APIs,  Cyber Source auth, Aurus Pay, AppleCare Insurance, Square Trade Insurance Build/Release and others: GitHub, JIRA, Confluence, JenkinsClient: Acesse Corporation, Redwood City, CA                                                    April, 2014 to June, 2015Role: Senior Project Manager/Agile Program Manager/Scrum MasterProject Description: Acesse has suite of online software products which are mainly used in China for content management, online marketing, Conference meetings and Mobile Chat related products Responsibilities:Set up the entire Jira processes, boards, trainings. Trained the entire organization on Agile and Scrum. Agile Program management for 4 of their scrum teams which were spread across Romania, Minneapolis USA, Redwood City USA and Hyderabad. Conducted monthly management meetings and helped then with metrics and analysis of all scrum teams with respect to schedule, delivery and quality/ Conducted all sprint ceremonies like Sprint Planning, daily stand up, reviews & retrospective meetings for all scrum teams Worked as co-product owner for some of their products and helped to build requirements and Traceability. Bridged the gap between PO & QA teams. Being a single point of contact for their 50 member offshore team, conducted daily review meetings, monthly project progress reviews and coordinated production releases.Environment: PHP, My SQL, MS SQL, Android and IOS  Client: Angie’s List, Indianapolis, IN, USA                                                       Sep, 2012 to March, 2014Role: Senior Project Manager/Scrum MasterProject Description: Angie's List is an American home services website. It is an online directory that allows users to read and publish crowd-sourced reviews of local businesses and contractors. Formerly a subscription-only service, Angie's List added a free membership tier in July 2016 Responsibilities: Set up the entire Jira processes, boards, trainings. Trained the entire organization on Agile and Scrum. Agile Program management for 3 of their scrum teams which were spread across Indianapolis & Palo Alto in USA. Conducted monthly management meetings and helped then with metrics and analysis of all scrum teams with respect to schedule, delivery and quality/ Conducted all sprint ceremonies like Sprint Planning, daily stand up, reviews & retrospective meetings for all scrum teamsEnvironment: C#, HTML5, CSS3, MS SQL Server 2010 Client: Serengeti Law a business of Thomson Reuters, Seattle, WA                Dec 2010 –August 2012Role: Senior Manager, Project ManagerProject Description: Serengeti Intelligence System – Analytics Dashboards  Serengeti Intelligence—a revolutionary way for law departments to easily access comparative and analytical information about your own performance—is ready to use within your Serengeti Tracker subscription at no additional charge. And, we do it right in the context of your matters, invoices, and workflows. Financial – Compare your spending and risk data against similarly situated law departments to give you insights into your performance  Operational – Assess the efficiency of your law department operations and management  Firm – Evaluate your law firm portfolio and the performance of your law firms  Evaluator – Improve analysis of law firm rates, rate increases, discounts and how your company’s bottom line is affectedResponsibilities:Preparing and maintaining project Management Plan. Managing the schedule and quality of the deliverables through day-to-day planning, tracking and monitoring of the project. Triaging and prioritizing the requirements with the designated Serengeti project/product Manager. Handling Estimations, Project Planning & Tracking Maintaining Quality Standards Interaction with the client and coordinating offshore activities. Maintaining billing & other finance details of the project Data analysis, RCA, CAPA & Reporting Risk Management Conducting Monthly and Quarterly Business reviews Building Proposals, SOW and working on Presales. Resource ManagementEnvironment: ASP.NET, SQL Server 2008, SSIS, SSAS, SSRS, XML Projects managed and delivered between Jan 2009 – July 2007 at Aditi Technologies as PROJECT MANAGER Client: Datacard Software India Pvt Ltd, Bangalore                                        Jan 2003 – July 2007Role: Java Developer, Performance Engineer, Lead in Java basedProduct Title  \t:  Affina – Smart Cards Life Cycle Managing SystemClient: Infosys Technologies Pvt Ltd, Bangalore                                August 2002 to January 2003Role: Software Automation Test EngineerProject Title   \t:  CAMPUS PIPELINE \tEnvironment \t: RTM Rational Robot, Clear Quest, and CP Table Builder.Client: Webwavez Technologies, Bangalore                                                   July 2000 to July 2002Role: Software EngineerEnvironment: Java Servlets, JSP, MSSQL, JavaScriptACHIEVEMENTS:Best Program Manager award from Client (Angie’s List – 2013) CEO, Aditi appreciations for contributions towards a Business Initiative in 2012.  “Best Mechanized Project” for a project Aditi Technologies, Bangalore ~“Best Performer” award in Aditi Technologies, Bangalore  ‘‘Most Significant Contributor’ (For EP3R1.1 release) award at Datacard Software India Pvt Ltd, Bangalore \n"
     ]
    }
   ],
   "source": [
    "# Get a list of all .docx resume files\n",
    "resumes = [f for f in os.listdir('./dataset/resumes') if f.endswith('.docx')]\n",
    "\n",
    "# Select a random .docx file\n",
    "random_resume = random.choice(resumes)\n",
    "\n",
    "# Create a Document object\n",
    "doc = Document(os.path.join('./dataset/resumes', random_resume))\n",
    "\n",
    "# Extract text\n",
    "resume = \"\"\n",
    "for para in doc.paragraphs:\n",
    "    resume += para.text\n",
    "    \n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Job Description**:\n",
    "\n",
    "Get a random JD from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Select a random JD from the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# random_jd = random.choice(df['data'])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m random_jd \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom JD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_jd)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# # Select a random JD from the dataset\n",
    "# random_jd = random.choice(df['data'])\n",
    "\n",
    "random_jd = (df[0])\n",
    "print(\"Random JD:\", random_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the corresponding perfect resume for the random JD\n",
    "perfect_resume = perfect_resumes[df['data'].tolist().index(random_jd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Develop a function to pre-process input text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY1-Fn97WgoN"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)\n",
    "\n",
    "    # Remove RT and cc\n",
    "    text = re.sub('RT|cc', ' ', text)\n",
    "\n",
    "    # Remove hashtags\n",
    "    text = re.sub('#\\S+', '', text)\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub('@\\S+', '  ', text)\n",
    "    \n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation from the text\n",
    "    text = re.sub('[^a-z]', ' ', text)\n",
    "    \n",
    "    # Remove numerical values from the text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4TR1IklWqp8"
   },
   "outputs": [],
   "source": [
    "# Convert the perfect_resume dictionary to a string\n",
    "perfect_resume_str = json.dumps(perfect_resume)\n",
    "\n",
    "# Apply preprocess function to CV, JD, and perfect resume\n",
    "input_CV = preprocess_text(resume)\n",
    "input_JD = preprocess_text(random_jd)\n",
    "perfect_resume_text = preprocess_text(perfect_resume_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Matching\n",
    "Using the trained model, we infer the document vectors for the resume and job description. Then, we calculate the cosine similarity between the two vectors to determine the match between the resume and the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def calculate_similarity(vector1, vector2):\n",
    "    return cosine_similarity([vector1], [vector2])[0][0]\n",
    "\n",
    "# Infer vectors for the random resume, random JD, and perfect resume\n",
    "random_resume_vector = model.infer_vector(word_tokenize(input_CV))\n",
    "random_jd_vector = model.infer_vector(word_tokenize(input_JD))\n",
    "perfect_resume_vector = model.infer_vector(word_tokenize(perfect_resume_text))\n",
    "\n",
    "# Calculate similarity scores\n",
    "random_resume_similarity = calculate_similarity(random_resume_vector, random_jd_vector)\n",
    "perfect_resume_similarity = calculate_similarity(perfect_resume_vector, random_jd_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig1 = go.Figure(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=random_resume_similarity * 100,\n",
    "    title={'text': \"Random Resume Matching Score (%)\"},\n",
    "    gauge={\n",
    "        'axis': {'range': [0, 100]},\n",
    "        'steps': [\n",
    "            {'range': [0, 50], 'color': \"#FFB6C1\"},\n",
    "            {'range': [50, 70], 'color': \"#FFFFE0\"},\n",
    "            {'range': [70, 100], 'color': \"#90EE90\"}\n",
    "        ],\n",
    "        'threshold': {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 100}\n",
    "    }\n",
    "))\n",
    "\n",
    "fig1.update_layout(width=400, height=400)  # Adjust the width and height as desired\n",
    "fig1.show()\n",
    "\n",
    "fig2 = go.Figure(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=perfect_resume_similarity * 100,\n",
    "    title={'text': \"Perfect Resume Matching Score (%)\"},\n",
    "    gauge={\n",
    "        'axis': {'range': [0, 100]},\n",
    "        'steps': [\n",
    "            {'range': [0, 50], 'color': \"#FFB6C1\"},\n",
    "            {'range': [50, 70], 'color': \"#FFFFE0\"},\n",
    "            {'range': [70, 100], 'color': \"#90EE90\"}\n",
    "        ],\n",
    "        'threshold': {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 100}\n",
    "    }\n",
    "))\n",
    "\n",
    "fig2.update_layout(width=400, height=400)  # Adjust the width and height as desired\n",
    "fig2.show()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRandom Resume Matching Score:\", random_resume_similarity * 100, \"%\")\n",
    "print(\"Perfect Resume Matching Score:\", perfect_resume_similarity * 100, \"%\")\n",
    "\n",
    "if random_resume_similarity < 0.5:\n",
    "    print(colored(\"Low chance, need to modify your CV!\", \"red\", attrs=[\"bold\"]))\n",
    "elif random_resume_similarity >= 0.5 and random_resume_similarity < 0.7:\n",
    "    print(colored(\"Good chance but you can improve further!\", \"yellow\", attrs=[\"bold\"]))\n",
    "else:\n",
    "    print(colored(\"Excellent! You can submit your CV.\", \"green\", attrs=[\"bold\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Run ATS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input JD Input in text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JD by input text:\n",
    "# jd = input(\"Paste your JD here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMuC1hBtWqmr"
   },
   "outputs": [],
   "source": [
    "# jd = \"\"\"\n",
    "# Proactively engage with clients in the branch and deliver an outstanding service experience through completing transactions and by identifying opportunities for advice, solutions, digital enablement and partner introductions\n",
    "# Focus on education and demonstration, leverage technology to deliver a memorable client experience, drive solutions and retain business\n",
    "# Contribute to team results by listening and spotting opportunities to offer additional advice, introduce clients to the capability of RBC partners, or personally fulfil client solutions\n",
    "# Proactively take ownership of resolving and preventing client banking problems\n",
    "# Cultivate and maintain relationships with partners to work as one RBC team\n",
    "# Manage risks by adhering to compliance routines, processes, and controls to protect client and shareholder interests while completing transactions\n",
    "# What do you need to succeed?\n",
    "\n",
    "# Must-have\n",
    "\n",
    "# Goal-oriented individual with a demonstrated passion for putting clients first.\n",
    "# Drive and self-motivation, as well as excellent communication skills and emotional intelligence\n",
    "# Digital literacy across a broad range of devices (i.e., smartphones, tablets, laptops, etc.)\n",
    "# Personal flexibility to work flex hours\n",
    "# Eagerness to learn and determination to succeed\n",
    "# Confidence and ability to learn financial concepts and willingness to obtain the Investment Funds in Canada or the Canadian Securities Course\n",
    "# Nice-to-have\n",
    "\n",
    "# Track record in building rapport and maintaining client relationships within the financial, service or retail industry\n",
    "# Mutual Funds accreditation\n",
    "# Is this job right for you? Check out our video and decide for yourself!\n",
    "\n",
    "# What’s in it for you?\n",
    "\n",
    "# We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual.\n",
    "\n",
    "# A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable\n",
    "# A world-class training program in financial services\n",
    "# Excellent career development and access to a variety of job opportunities across business and geographies\n",
    "# Leaders who support your development through coaching and managing opportunities\n",
    "# Work in a dynamic, collaborative, progressive, and high-performing team\n",
    "# We also strive to provide an accessible candidate experience for our prospective employees with different abilities. Please let us know if you need any accommodations during the recruitment process.\n",
    "\n",
    "# Join our Talent Community\n",
    "\n",
    "# Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you.\n",
    "\n",
    "# Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc.com/careers.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model\n",
    "# def evaluate_model(perfect_resumes, model_predictions):\n",
    "#     # Assuming model_predictions is a list of tuples (resume, score, rank)\n",
    "#     true_positives = 0\n",
    "#     false_positives = 0\n",
    "#     false_negatives = 0\n",
    "\n",
    "#     for perfect_resume, (predicted_resume, score, rank) in zip(perfect_resumes, model_predictions):\n",
    "#         # Define a threshold for considering a resume as a positive match\n",
    "#         threshold = 0.7  # Adjust this value as needed\n",
    "\n",
    "#         # Check if the predicted resume matches the perfect resume\n",
    "#         if score >= threshold:\n",
    "#             if perfect_resume == predicted_resume:\n",
    "#                 true_positives += 1\n",
    "#             else:\n",
    "#                 false_positives += 1\n",
    "#         else:\n",
    "#             if perfect_resume != predicted_resume:\n",
    "#                 false_negatives += 1\n",
    "\n",
    "#     precision = precision_score(true_positives, true_positives + false_positives)\n",
    "#     recall = recall_score(true_positives, true_positives + false_negatives)\n",
    "#     f1 = f1_score(true_positives, true_positives + false_positives + false_negatives)\n",
    "\n",
    "#     print(f\"Precision: {precision:.2f}\")\n",
    "#     print(f\"Recall: {recall:.2f}\")\n",
    "#     print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "# # Save perfect resumes to a file\n",
    "# with open('perfect_resumes.txt', 'w', encoding='utf-8') as f:\n",
    "#     for resume in perfect_resumes:\n",
    "#         resume_str = str(resume)\n",
    "#         f.write(resume_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlbOvGLfWqsd"
   },
   "outputs": [],
   "source": [
    "# # Model evaluation\n",
    "# model = Doc2Vec.load('cv_job_maching.model')\n",
    "# v1 = model.infer_vector(input_CV.split())\n",
    "# v2 = model.infer_vector(input_JD.split())\n",
    "# similarity = 100*(np.dot(np.array(v1), np.array(v2))) / (norm(np.array(v1)) * norm(np.array(v2)))\n",
    "# print(round(similarity, 2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
