{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Load datasets\n",
        "jd_df = pd.read_csv('/content/DataScientist.csv')\n",
        "resume_df = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "\n",
        "# Preprocessing text\n",
        "import re\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    return text\n",
        "\n",
        "jd_df['Job Description'] = jd_df['Job Description'].apply(preprocess)\n",
        "resume_df['Resume'] = resume_df['Resume'].apply(preprocess)\n",
        "\n",
        "# Combine and tag documents\n",
        "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(resume_df['Resume'])]\n",
        "jd_documents = [TaggedDocument(doc.split(), ['jd_{}'.format(i)]) for i, doc in enumerate(jd_df['Job Description'])]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model_d2v = Doc2Vec(documents + jd_documents, vector_size=50, window=2, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE3WBEDERk1s",
        "outputId": "5f1cc096-d469-46ff-e343-06626b5f1287"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_vectors = [model_d2v.infer_vector(doc.words) for doc in documents]\n",
        "jd_vectors = [model_d2v.infer_vector(doc.words) for doc in jd_documents]\n"
      ],
      "metadata": {
        "id": "2heJhOyWShab"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume first job description matches with the first 100 resumes (simplification)\n",
        "labels = [1 if i < 10 else 0 for i in range(len(resume_vectors))]  # Simplified example\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(resume_vectors, labels, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "2LtyPRaYSjkM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the enhanced neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=50, activation='relu'))  # Increased the number of neurons\n",
        "model.add(BatchNormalization())  # Batch Normalization layer\n",
        "model.add(Dropout(0.3))  # Dropout layer with 30% rate\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer remains the same\n",
        "\n",
        "# Compile the model with advanced optimizer\n",
        "optimizer = Adam(learning_rate=0.001)  # Customizable learning rate\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Train Accuracy:', train_acc)\n",
        "print('Test Accuracy:', test_acc)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik2AGtLvTGp1",
        "outputId": "162f3fd5-1446-4ec8-89f7-20cb52dbf2ec"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 4s 28ms/step - loss: 0.8202 - accuracy: 0.5022 - val_loss: 0.6503 - val_accuracy: 0.8547\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.7148 - accuracy: 0.5914 - val_loss: 0.5869 - val_accuracy: 0.9723\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.6117 - accuracy: 0.6880 - val_loss: 0.5280 - val_accuracy: 0.9862\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5512 - accuracy: 0.7370 - val_loss: 0.4831 - val_accuracy: 0.9896\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5010 - accuracy: 0.7845 - val_loss: 0.4462 - val_accuracy: 0.9896\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4406 - accuracy: 0.8410 - val_loss: 0.3816 - val_accuracy: 0.9896\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4045 - accuracy: 0.8796 - val_loss: 0.3309 - val_accuracy: 0.9896\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3561 - accuracy: 0.8975 - val_loss: 0.3049 - val_accuracy: 0.9896\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3070 - accuracy: 0.9406 - val_loss: 0.2547 - val_accuracy: 0.9896\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2596 - accuracy: 0.9584 - val_loss: 0.2226 - val_accuracy: 0.9896\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2392 - accuracy: 0.9599 - val_loss: 0.1935 - val_accuracy: 0.9896\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2127 - accuracy: 0.9703 - val_loss: 0.1605 - val_accuracy: 0.9896\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1849 - accuracy: 0.9777 - val_loss: 0.1510 - val_accuracy: 0.9896\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1689 - accuracy: 0.9747 - val_loss: 0.1362 - val_accuracy: 0.9896\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1432 - accuracy: 0.9837 - val_loss: 0.1246 - val_accuracy: 0.9896\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1332 - accuracy: 0.9807 - val_loss: 0.1085 - val_accuracy: 0.9896\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1014 - accuracy: 0.9851 - val_loss: 0.1006 - val_accuracy: 0.9862\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1095 - accuracy: 0.9822 - val_loss: 0.0939 - val_accuracy: 0.9862\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.0873 - accuracy: 0.9896 - val_loss: 0.0860 - val_accuracy: 0.9862\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.0819 - accuracy: 0.9866 - val_loss: 0.0845 - val_accuracy: 0.9758\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.0822 - accuracy: 0.9822 - val_loss: 0.0821 - val_accuracy: 0.9827\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.0759 - accuracy: 0.9896 - val_loss: 0.0875 - val_accuracy: 0.9758\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.0779 - accuracy: 0.9837 - val_loss: 0.0900 - val_accuracy: 0.9758\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9896 - val_loss: 0.0890 - val_accuracy: 0.9758\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.0629 - accuracy: 0.9911 - val_loss: 0.0970 - val_accuracy: 0.9689\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9926 - val_loss: 0.0983 - val_accuracy: 0.9689\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.9881 - val_loss: 0.1047 - val_accuracy: 0.9689\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.0585 - accuracy: 0.9837 - val_loss: 0.1035 - val_accuracy: 0.9689\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.0406 - accuracy: 0.9941 - val_loss: 0.1038 - val_accuracy: 0.9689\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.0448 - accuracy: 0.9911 - val_loss: 0.1144 - val_accuracy: 0.9654\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9881\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.9654\n",
            "Train Accuracy: 0.9881129264831543\n",
            "Test Accuracy: 0.9653978943824768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_resumes_for_jd(jd_text, top_n=5):\n",
        "    # Preprocess the JD text\n",
        "    jd_text_clean = preprocess(jd_text)\n",
        "\n",
        "    # Convert JD text to a vector\n",
        "    jd_vector = model_d2v.infer_vector(jd_text_clean.split()).reshape(1, -1)\n",
        "\n",
        "    # Create a matrix of JD vector repeated for each resume in X_test\n",
        "    jd_matrix = np.repeat(jd_vector, len(X_test), axis=0)\n",
        "\n",
        "    # Compute the model's predictions for these vectors\n",
        "    similarity_scores = model.predict(jd_matrix).flatten()\n",
        "\n",
        "    # Get the indices of the top matching resumes\n",
        "    top_indices = np.argsort(similarity_scores)[-top_n:][::-1]\n",
        "\n",
        "    # Retrieve the top matching resumes\n",
        "    return resume_df.iloc[top_indices]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "teO_X37cV84F"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example job description input\n",
        "input_jd = jd_df['Job Description'].iloc[1]\n",
        "print(sample_jd)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxkcNTuqV91e",
        "outputId": "be3df81f-a306-4dcb-ac8c-86a579313991"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at noom we use scientifically proven methods to help our users create healthier lifestyles and manage important conditions like type ii diabetes obesity and hypertension our engineering team is at the forefront of this challenge solving complex technical and ux problems on our mobile apps that center around habits behavior and lifestyle we are looking for a data scientist to join our data team and help us ensure that we apply the best approaches to data analysis and research artificial intelligence and machine learning what you ll like about us we work on problems that affect the lives of real people our users depend on us to make positive changes to their health and their lives we base our work on scientifically proven peer reviewed methodologies that are designed by medical professionals we are a data driven company through and through we re a respectful diverse and dynamic environment in which engineering is a first class citizen and where you ll be able to work on a variety of interesting problems that affect the lives of real people we offer a generous budget for personal development expenses like training courses conferences and books you ll get three weeks paid vacation and a flexible work policy that is remote and family friendly about 50 of our engineering team is fully remote we worry about results not time spent in seats what we ll like about you you have 4 years of experience as a data scientist or data analyst in a similarly sized organization with a proven record of analysis and research that positively impacts your team you possess excellent communication skills and the ability to clearly communicate technical concepts to a non technical audience you possess excellent sql relational algebra skills ideally with at least a basic knowledge of how different types of databases e g column vs row storage work you have a superior knowledge of statistical analysis methods such as input selection logistic and standard regression etc you are comfortable writing python code and have good working knowledge of pandas and numpy we don t expect you to write production quality code but you should have some programming experience you are comfortable with at least medium data technologies and how to transcend the memory bound nature of most analytics tools \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_matching_resumes = get_top_resumes_for_jd(input_jd)\n",
        "print(top_matching_resumes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sofXiFEIlRdS",
        "outputId": "5d482646-2f25-4eb0-e859-bdb693a39159"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "               Category                                             Resume\n",
            "288  Health and fitness  education details may 2014 diploma nutrition e...\n",
            "143       Web Designing  education details january 2016 b sc informatio...\n",
            "91             Advocate  skills legal writing efficient researcher lega...\n",
            "92             Advocate  good grasping quality and skillful work educat...\n",
            "93             Advocate  â hard working â quick learnereducation detail...\n"
          ]
        }
      ]
    }
  ]
}